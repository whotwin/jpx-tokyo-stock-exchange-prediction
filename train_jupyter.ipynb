{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JPX Stock Prediction - 30-Day Horizon with Expanding Window Training\n",
    "\n",
    "This notebook implements a stock return prediction model for JPX Tokyo Stock Exchange.\n",
    "\n",
    "## Strategy\n",
    "- Use all available data from 2017 onwards\n",
    "- Expanding window: train on past data, predict next year\n",
    "- 30-day forward return prediction\n",
    "- LightGBM regression + classification hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OUTPUT_DIR = \"output_train_30d\"\n",
    "TEST_YEAR = 2021\n",
    "ROLL_TRAIN_YEARS = 2\n",
    "TARGET_HORIZON = 30\n",
    "TOP_K = 200\n",
    "BOTTOM_K = 200\n",
    "\n",
    "TRADING_COST_RATE = 0.0004\n",
    "SLIPPAGE_RATE = 0.0002\n",
    "\n",
    "PARAM_GRID = {\n",
    "    \"n_estimators\": [300, 500],\n",
    "    \"learning_rate\": [0.01, 0.02],\n",
    "    \"max_depth\": [6, 8],\n",
    "    \"num_leaves\": [15, 31],\n",
    "}\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(msg):\n",
    "    print(f\"[INFO] {msg}\")\n",
    "\n",
    "def to_num(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_sources(data_dir=\"train_files\"):\n",
    "    sources = {}\n",
    "\n",
    "    # Load stock prices\n",
    "    stock_prices = pd.read_csv(os.path.join(data_dir, \"stock_prices.csv\"))\n",
    "    stock_prices = to_num(stock_prices, [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \n",
    "                                   \"AdjustmentFactor\", \"ExpectedDividend\", \"Target\", \"SupervisionFlag\"])\n",
    "    stock_prices[\"Date\"] = pd.to_datetime(stock_prices[\"Date\"])\n",
    "    sources[\"stock_prices\"] = stock_prices\n",
    "\n",
    "    # Load stock list\n",
    "    if os.path.exists(os.path.join(data_dir, \"stock_list.csv\")):\n",
    "        stock_list = pd.read_csv(os.path.join(data_dir, \"stock_list.csv\"),\n",
    "                                 usecols=[\"SecuritiesCode\", \"MarketCapitalization\", \"33SectorName\", \"NewMarketSegment\"])\n",
    "        sources[\"stock_list\"] = stock_list\n",
    "\n",
    "    # Load secondary stock prices\n",
    "    if os.path.exists(os.path.join(data_dir, \"secondary_stock_prices.csv\")):\n",
    "        secondary = pd.read_csv(os.path.join(data_dir, \"secondary_stock_prices.csv\"))\n",
    "        secondary = to_num(secondary, [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"AdjustmentFactor\"])\n",
    "        secondary[\"Date\"] = pd.to_datetime(secondary[\"Date\"])\n",
    "        sources[\"secondary_stock_prices\"] = secondary\n",
    "\n",
    "    # Load options\n",
    "    if os.path.exists(os.path.join(data_dir, \"options.csv\")):\n",
    "        opts = pd.read_csv(os.path.join(data_dir, \"options.csv\"))\n",
    "        opts = to_num(opts, [\"ImpliedVolatility\", \"TradingVolume\", \"OpenInterest\", \"SettlementPrice\", \"BaseVolatility\"])\n",
    "        opts[\"Date\"] = pd.to_datetime(opts[\"Date\"])\n",
    "        sources[\"options\"] = opts\n",
    "\n",
    "    # Load trades\n",
    "    if os.path.exists(os.path.join(data_dir, \"trades.csv\")):\n",
    "        trades = pd.read_csv(os.path.join(data_dir, \"trades.csv\"))\n",
    "        trades = to_num(trades, [\"Individual\", \"Foreigners\", \"SecuritiesCos\", \"InvestmentTrusts\", \n",
    "                               \"InsuranceCos\", \"CityBKs\", \"RegionalBKs\", \"TrustBanks\"])\n",
    "        trades[\"Date\"] = pd.to_datetime(trades[\"Date\"])\n",
    "        sources[\"trades\"] = trades\n",
    "\n",
    "    # Load financials\n",
    "    if os.path.exists(os.path.join(data_dir, \"financials.csv\")):\n",
    "        financials = pd.read_csv(os.path.join(data_dir, \"financials.csv\"), low_memory=False)\n",
    "        financials = to_num(financials, [\"NetSales\", \"OperatingProfit\", \"OrdinaryProfit\", \"Profit\", \n",
    "                                        \"TotalAssets\", \"Equity\", \"EquityToAssetRatio\", \"EarningsPerShare\", \n",
    "                                        \"ForecastedEarningsPerShare\"])\n",
    "        financials[\"Date\"] = pd.to_datetime(financials[\"Date\"])\n",
    "        sources[\"financials\"] = financials\n",
    "\n",
    "    log(f\"Loaded data sources: {list(sources.keys())}\")\n",
    "    return sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_features(px, stock_list=None):\n",
    "    \"\"\"Extract stock-level features from price data\"\"\"\n",
    "    px = px.sort_values([\"SecuritiesCode\", \"Date\"]).reset_index(drop=True)\n",
    "    g = px.groupby(\"SecuritiesCode\", sort=False)\n",
    "    c = px[\"Close\"].replace(0, np.nan)\n",
    "\n",
    "    # Returns\n",
    "    for w in [1, 2, 3, 5, 10, 20]:\n",
    "        px[f\"stk_ret_{w}\"] = g[\"Close\"].pct_change(w)\n",
    "\n",
    "    # Log returns\n",
    "    px[\"stk_logret_1\"] = np.log(c).groupby(px[\"SecuritiesCode\"]).diff(1)\n",
    "\n",
    "    # Spread features\n",
    "    px[\"stk_hl_spread\"] = (px[\"High\"] - px[\"Low\"]) / c\n",
    "    px[\"stk_oc_spread\"] = (px[\"Close\"] - px[\"Open\"]) / px[\"Open\"].replace(0, np.nan)\n",
    "\n",
    "    # Volume change\n",
    "    px[\"stk_volume_chg_1\"] = g[\"Volume\"].pct_change(1)\n",
    "\n",
    "    # Volatility\n",
    "    for w in [5, 10, 20]:\n",
    "        px[f\"stk_vol_{w}\"] = g[\"stk_logret_1\"].transform(lambda x: x.rolling(w, min_periods=w).std())\n",
    "\n",
    "    # Rolling mean returns\n",
    "    for w in [5, 10, 20]:\n",
    "        px[f\"stk_ret_mean_{w}\"] = g[\"stk_ret_1\"].transform(lambda x: x.rolling(w, min_periods=w).mean())\n",
    "\n",
    "    # Moving averages\n",
    "    for w in [5, 10, 20]:\n",
    "        ma = g[\"Close\"].transform(lambda x: x.rolling(w, min_periods=w).mean())\n",
    "        px[f\"stk_close_to_ma_{w}\"] = px[\"Close\"] / ma - 1\n",
    "\n",
    "    # Volume to MA\n",
    "    for w in [5, 10, 20]:\n",
    "        vma = g[\"Volume\"].transform(lambda x: x.rolling(w, min_periods=w).mean())\n",
    "        px[f\"stk_volume_to_ma_{w}\"] = px[\"Volume\"] / vma - 1\n",
    "\n",
    "    # Skewness\n",
    "    px[\"stk_skew_20\"] = g[\"stk_logret_1\"].transform(lambda x: x.rolling(20, min_periods=20).skew())\n",
    "\n",
    "    # Day of week and month\n",
    "    px[\"stk_dayofweek\"] = px[\"Date\"].dt.dayofweek\n",
    "    px[\"stk_month\"] = px[\"Date\"].dt.month\n",
    "\n",
    "    # Expected dividend\n",
    "    px[\"stk_expected_dividend\"] = px[\"ExpectedDividend\"].fillna(0)\n",
    "\n",
    "    # Supervision flag\n",
    "    px[\"stk_supervision_flag\"] = px[\"SupervisionFlag\"].astype(str).str.lower().eq(\"true\").astype(int)\n",
    "\n",
    "    # Add market cap and sector features\n",
    "    if stock_list is not None:\n",
    "        stock_list = stock_list.copy()\n",
    "        px = px.merge(\n",
    "            stock_list[[\"SecuritiesCode\", \"MarketCapitalization\", \"33SectorName\", \"NewMarketSegment\"]],\n",
    "            on=\"SecuritiesCode\", how=\"left\"\n",
    "        )\n",
    "        px[\"stk_mcap\"] = np.log(px[\"MarketCapitalization\"].fillna(1e8) / 1e8 + 1)\n",
    "        px[\"stk_sector\"] = pd.Categorical(px[\"33SectorName\"]).codes\n",
    "        px[\"stk_market_segment\"] = pd.Categorical(px[\"NewMarketSegment\"]).codes\n",
    "        px = px.drop(columns=[\"MarketCapitalization\", \"33SectorName\", \"NewMarketSegment\"], errors=\"ignore\")\n",
    "\n",
    "    return px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def options_features(opts):\n",
    "    \"\"\"Extract options features\"\"\"\n",
    "    if opts is None or opts.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    opts = opts.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    if \"ImpliedVolatility\" in opts.columns:\n",
    "        iv = opts.groupby(\"Date\")[\"ImpliedVolatility\"].mean().reset_index()\n",
    "        iv.columns = [\"Date\", \"iv_avg\"]\n",
    "        return iv\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def trades_features(trades_df):\n",
    "    \"\"\"Extract trades features\"\"\"\n",
    "    if trades_df is None or trades_df.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    trades_df = trades_df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    investor_cols = [\"Individual\", \"Foreigners\", \"SecuritiesCos\", \"InvestmentTrusts\"]\n",
    "    available_cols = [c for c in investor_cols if c in trades_df.columns]\n",
    "\n",
    "    if not available_cols:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    result = trades_df.groupby(\"Date\")[available_cols].mean().reset_index()\n",
    "    result.columns = [\"Date\"] + [f\"trd_{c.lower()}\" for c in available_cols]\n",
    "\n",
    "    return result\n",
    "\n",
    "def financials_features(fn):\n",
    "    \"\"\"Extract financials features\"\"\"\n",
    "    if fn is None or fn.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    fn = fn.sort_values([\"SecuritiesCode\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "    num_cols = [\"NetSales\", \"OperatingProfit\", \"OrdinaryProfit\", \"Profit\", \"TotalAssets\", \"Equity\"]\n",
    "    available_cols = [c for c in num_cols if c in fn.columns]\n",
    "\n",
    "    if not available_cols:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    fn[available_cols] = fn.groupby(\"SecuritiesCode\")[available_cols].ffill()\n",
    "    result = fn.groupby(\"SecuritiesCode\", as_index=False)[available_cols].last()\n",
    "    result.columns = [\"SecuritiesCode\"] + [f\"fin_{c.lower()}\" for c in available_cols]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_table(sources, start_date=None, end_date=None):\n",
    "    \"\"\"Build full feature table\"\"\"\n",
    "    prices = sources[\"stock_prices\"].copy()\n",
    "\n",
    "    if start_date:\n",
    "        prices = prices[prices[\"Date\"] >= pd.to_datetime(start_date)]\n",
    "    if end_date:\n",
    "        prices = prices[prices[\"Date\"] <= pd.to_datetime(end_date)]\n",
    "\n",
    "    df = prices[[\"Date\", \"SecuritiesCode\", \"Close\", \"Volume\", \"High\", \"Low\", \"Open\", \n",
    "                 \"ExpectedDividend\", \"SupervisionFlag\"]].copy()\n",
    "\n",
    "    # Stock features\n",
    "    stock_list = sources.get(\"stock_list\", None)\n",
    "    px_with_features = stock_features(prices, stock_list=stock_list)\n",
    "\n",
    "    stock_cols = [c for c in px_with_features.columns if c.startswith(\"stk_\")]\n",
    "    df = df.merge(px_with_features[[\"Date\", \"SecuritiesCode\"] + stock_cols],\n",
    "                  on=[\"Date\", \"SecuritiesCode\"], how=\"left\")\n",
    "\n",
    "    # Options features\n",
    "    if \"options\" in sources:\n",
    "        opt_feat = options_features(sources[\"options\"])\n",
    "        if not opt_feat.empty:\n",
    "            df = df.merge(opt_feat, on=\"Date\", how=\"left\")\n",
    "\n",
    "    # Trades features\n",
    "    if \"trades\" in sources:\n",
    "        trd_feat = trades_features(sources[\"trades\"])\n",
    "        if not trd_feat.empty:\n",
    "            df = df.merge(trd_feat, on=\"Date\", how=\"left\")\n",
    "\n",
    "    # Financials features\n",
    "    if \"financials\" in sources:\n",
    "        fin_feat = financials_features(sources[\"financials\"])\n",
    "        if not fin_feat.empty:\n",
    "            df = df.merge(fin_feat, on=\"SecuritiesCode\", how=\"left\")\n",
    "\n",
    "    # Get feature columns\n",
    "    non_feature_cols = [\"Date\", \"SecuritiesCode\", \"Close\", \"Volume\", \"High\", \"Low\", \"Open\", \n",
    "                        \"ExpectedDividend\", \"SupervisionFlag\"]\n",
    "    feature_cols = [c for c in df.columns if c not in non_feature_cols]\n",
    "\n",
    "    # Shift features by 1 to avoid leakage\n",
    "    for col in feature_cols:\n",
    "        df[col] = df.groupby(\"SecuritiesCode\", sort=False)[col].shift(1)\n",
    "\n",
    "    df = df.dropna(subset=[\"Date\", \"SecuritiesCode\", \"Close\"])\n",
    "    df[feature_cols] = df[feature_cols].fillna(0)\n",
    "\n",
    "    log(f\"Built feature table: {len(df)} rows, {len(feature_cols)} features\")\n",
    "\n",
    "    return df, feature_cols\n",
    "\n",
    "def build_30d_labels(stock_prices):\n",
    "    \"\"\"Build 30-day forward return labels\"\"\"\n",
    "    px = stock_prices[[\"Date\", \"SecuritiesCode\", \"Close\"]].copy()\n",
    "    px[\"Close\"] = pd.to_numeric(px[\"Close\"], errors=\"coerce\")\n",
    "    px = px.sort_values([\"SecuritiesCode\", \"Date\"]).reset_index(drop=True)\n",
    "\n",
    "    px[\"target_30d\"] = px.groupby(\"SecuritiesCode\")[\"Close\"].shift(-30) / px[\"Close\"] - 1.0\n",
    "\n",
    "    return px[[\"Date\", \"SecuritiesCode\", \"target_30d\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    log(\"Loading all data sources...\")\n",
    "    sources = load_data_sources(\"train_files\")\n",
    "\n",
    "    log(\"Building full feature table...\")\n",
    "    full_df, feature_cols = build_feature_table(\n",
    "        sources=sources,\n",
    "        start_date=\"2017-01-04\",\n",
    "        end_date=\"2021-12-03\",\n",
    "    )\n",
    "\n",
    "    labels = build_30d_labels(sources[\"stock_prices\"])\n",
    "    full_df = full_df.merge(labels, on=[\"Date\", \"SecuritiesCode\"], how=\"left\")\n",
    "    full_df = full_df.sort_values([\"Date\", \"SecuritiesCode\"]).reset_index(drop=True)\n",
    "    log(f\"Loaded: {len(full_df)} rows\")\n",
    "\n",
    "    return full_df, feature_cols\n",
    "\n",
    "def load_dataset():\n",
    "    full_df, feature_cols = load_all_data()\n",
    "    target_col = \"target_30d\"\n",
    "    data = full_df[[\"Date\", \"SecuritiesCode\"] + feature_cols + [target_col]].copy()\n",
    "    log(f\"Features: {len(feature_cols)}\")\n",
    "    log(f\"Target: {target_col}\")\n",
    "    return data, feature_cols, target_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(df, feature_cols, target_col):\n",
    "    log(\"Starting hyperparameter tuning...\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"Year\"] = df[\"Date\"].dt.year\n",
    "\n",
    "    tuning_pairs = [(2019, 2020)]\n",
    "\n",
    "    best_params = None\n",
    "    best_score = -np.inf\n",
    "\n",
    "    param_combinations = []\n",
    "    for n_est in PARAM_GRID[\"n_estimators\"]:\n",
    "        for lr in PARAM_GRID[\"learning_rate\"]:\n",
    "            for depth in PARAM_GRID[\"max_depth\"]:\n",
    "                for leaves in PARAM_GRID[\"num_leaves\"]:\n",
    "                    param_combinations.append({\n",
    "                        \"n_estimators\": n_est,\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"max_depth\": depth,\n",
    "                        \"num_leaves\": leaves,\n",
    "                    })\n",
    "\n",
    "    log(f\"Testing {min(12, len(param_combinations))} param combinations...\")\n",
    "\n",
    "    sample_size = min(30000, len(df[df[target_col].notna()]))\n",
    "    df_sample = df[df[target_col].notna()].sample(n=sample_size, random_state=42)\n",
    "\n",
    "    for params in param_combinations[:12]:\n",
    "        scores = []\n",
    "\n",
    "        for train_year, test_year in tuning_pairs:\n",
    "            train_df = df_sample[(df_sample[\"Year\"] >= train_year) & (df_sample[\"Year\"] < test_year)]\n",
    "            test_df = df_sample[df_sample[\"Year\"] == test_year]\n",
    "\n",
    "            if len(train_df) < 500 or len(test_df) < 50:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                model = LGBMRegressor(\n",
    "                    n_estimators=params[\"n_estimators\"],\n",
    "                    learning_rate=params[\"learning_rate\"],\n",
    "                    max_depth=params[\"max_depth\"],\n",
    "                    num_leaves=params[\"num_leaves\"],\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1,\n",
    "                    verbose=-1,\n",
    "                )\n",
    "                model.fit(train_df[feature_cols], train_df[target_col].values)\n",
    "\n",
    "                pred = model.predict(test_df[feature_cols])\n",
    "                spearman = pd.Series(test_df[target_col].values).corr(pd.Series(pred), method=\"spearman\")\n",
    "\n",
    "                if not np.isnan(spearman):\n",
    "                    scores.append(spearman)\n",
    "\n",
    "                del model\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if scores:\n",
    "            avg_score = np.mean(scores)\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                best_params = params\n",
    "\n",
    "    if best_params:\n",
    "        log(f\"Best params: {best_params}, Score: {best_score:.4f}\")\n",
    "    else:\n",
    "        log(\"Using default parameters\")\n",
    "        best_params = {\"n_estimators\": 500, \"learning_rate\": 0.02, \"max_depth\": 8, \"num_leaves\": 31}\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm_regressor(train_df, feature_cols, target_col, params):\n",
    "    model = LGBMRegressor(\n",
    "        n_estimators=params.get(\"n_estimators\", 500),\n",
    "        learning_rate=params.get(\"learning_rate\", 0.02),\n",
    "        max_depth=params.get(\"max_depth\", 8),\n",
    "        num_leaves=params.get(\"num_leaves\", 31),\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1,\n",
    "    )\n",
    "    model.fit(train_df[feature_cols], train_df[target_col].values)\n",
    "    return model\n",
    "\n",
    "def fit_lgbm_classifier(train_df, feature_cols, target_col):\n",
    "    y_binary = (train_df[target_col] > 0).astype(int)\n",
    "\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.02,\n",
    "        max_depth=6,\n",
    "        num_leaves=15,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1,\n",
    "    )\n",
    "    model.fit(train_df[feature_cols], y_binary)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_expanding_window(df, feature_cols, target_col, tuned_params):\n",
    "    log(f\"Running all-data-for-training, 2021-only prediction...\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"Year\"] = df[\"Date\"].dt.year\n",
    "\n",
    "    pred_parts = []\n",
    "\n",
    "    test_year = 2021\n",
    "    train_df = df[(df[\"Year\"] < test_year) & df[target_col].notna()].copy()\n",
    "    test_df = df[(df[\"Year\"] == test_year) & df[target_col].notna()].copy()\n",
    "\n",
    "    if train_df.empty or test_df.empty:\n",
    "        log(f\"  Year {test_year}: No data, skipping\")\n",
    "    else:\n",
    "        log(f\"  Year {test_year}: Train {len(train_df):,} (all prior years), Test {len(test_df):,}\")\n",
    "\n",
    "        model_reg = fit_lgbm_regressor(train_df, feature_cols, target_col, tuned_params)\n",
    "        model_cls = fit_lgbm_classifier(train_df, feature_cols, target_col)\n",
    "\n",
    "        reg_pred = model_reg.predict(test_df[feature_cols])\n",
    "        cls_prob = model_cls.predict_proba(test_df[feature_cols])[:, 1]\n",
    "\n",
    "        hybrid_pred = reg_pred * (2 * cls_prob - 1)\n",
    "\n",
    "        out = test_df[[\"Date\", \"SecuritiesCode\", target_col]].copy()\n",
    "        out = out.rename(columns={target_col: \"y_true\"})\n",
    "        out[\"pred_reg\"] = reg_pred\n",
    "        out[\"pred_prob\"] = cls_prob\n",
    "        out[\"pred\"] = hybrid_pred\n",
    "        out[\"train_year\"] = test_year - 1\n",
    "        pred_parts.append(out)\n",
    "\n",
    "        del model_reg, model_cls\n",
    "        gc.collect()\n",
    "\n",
    "    if pred_parts:\n",
    "        pred = pd.concat(pred_parts, ignore_index=True).sort_values([\"Date\", \"SecuritiesCode\"]).reset_index(drop=True)\n",
    "    else:\n",
    "        pred = pd.DataFrame(columns=[\"Date\", \"SecuritiesCode\", \"y_true\", \"pred\", \"train_year\"])\n",
    "\n",
    "    log(f\"Total predictions: {len(pred):,}\")\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_portfolio(pred_df):\n",
    "    if pred_df.empty:\n",
    "        return {\"num_days\": 0, \"sharpe\": np.nan, \"hit_ratio\": np.nan, \"spread\": np.nan}\n",
    "\n",
    "    pred_df = pred_df.sort_values(\"Date\").reset_index(drop=True)\n",
    "    dates = sorted(pred_df[\"Date\"].unique())\n",
    "\n",
    "    # Monthly rebalancing\n",
    "    monthly_dates = []\n",
    "    current_year_month = None\n",
    "    for d in dates:\n",
    "        dt = pd.to_datetime(d)\n",
    "        year_month = (dt.year, dt.month)\n",
    "        if year_month != current_year_month:\n",
    "            monthly_dates.append(d)\n",
    "            current_year_month = year_month\n",
    "\n",
    "    daily_results = []\n",
    "    prev_top = set()\n",
    "    prev_bottom = set()\n",
    "\n",
    "    for rebal_date in monthly_dates:\n",
    "        day_pred = pred_df[pred_df[\"Date\"] == rebal_date].copy()\n",
    "        if len(day_pred) < TOP_K + BOTTOM_K:\n",
    "            continue\n",
    "\n",
    "        sorted_pred = day_pred.sort_values(\"pred\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "        top200 = set(sorted_pred.head(TOP_K)[\"SecuritiesCode\"].astype(int).tolist())\n",
    "        bottom200 = set(sorted_pred.tail(BOTTOM_K)[\"SecuritiesCode\"].astype(int).tolist())\n",
    "\n",
    "        turnover = len(top200 - prev_top) + len(bottom200 - prev_bottom)\n",
    "        turnover = turnover / (TOP_K + BOTTOM_K)\n",
    "\n",
    "        prev_top = top200\n",
    "        prev_bottom = bottom200\n",
    "\n",
    "        top_ret = sorted_pred.head(TOP_K)[\"y_true\"].mean()\n",
    "        bottom_ret = sorted_pred.tail(BOTTOM_K)[\"y_true\"].mean()\n",
    "        spread = top_ret - bottom_ret\n",
    "\n",
    "        top_correct = (sorted_pred.head(TOP_K)[\"y_true\"] > 0).sum()\n",
    "        bottom_correct = (sorted_pred.tail(BOTTOM_K)[\"y_true\"] < 0).sum()\n",
    "        hit = (top_correct + bottom_correct) / (TOP_K + BOTTOM_K)\n",
    "\n",
    "        daily_results.append({\n",
    "            \"date\": rebal_date,\n",
    "            \"spread\": spread,\n",
    "            \"turnover\": turnover,\n",
    "            \"hit_ratio\": hit,\n",
    "        })\n",
    "\n",
    "    if not daily_results:\n",
    "        return {\"num_days\": 0, \"sharpe\": np.nan, \"hit_ratio\": np.nan, \"spread\": np.nan}\n",
    "\n",
    "    daily_df = pd.DataFrame(daily_results)\n",
    "    daily_df[\"spread_after_cost\"] = daily_df[\"spread\"] - daily_df[\"turnover\"] * (TRADING_COST_RATE + SLIPPAGE_RATE) * 2\n",
    "\n",
    "    avg_spread = daily_df[\"spread_after_cost\"].mean()\n",
    "    std_spread = daily_df[\"spread_after_cost\"].std()\n",
    "    sharpe = (avg_spread / std_spread * np.sqrt(4)) if std_spread > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"num_days\": len(daily_df),\n",
    "        \"sharpe\": float(sharpe),\n",
    "        \"hit_ratio\": float(daily_df[\"hit_ratio\"].mean()),\n",
    "        \"spread\": float(daily_df[\"spread\"].sum()),\n",
    "        \"daily_df\": daily_df,\n",
    "    }\n",
    "\n",
    "def evaluate_predictions(pred_df):\n",
    "    if pred_df.empty:\n",
    "        return {\"rmse\": np.nan, \"spearman\": np.nan, \"hit\": np.nan}\n",
    "\n",
    "    y = pred_df[\"y_true\"].values\n",
    "    p = pred_df[\"pred\"].values\n",
    "\n",
    "    rmse = float(np.sqrt(mean_squared_error(y, p)))\n",
    "    spearman = float(pd.Series(y).corr(pd.Series(p), method=\"spearman\"))\n",
    "    hit = float(np.mean(np.sign(y) == np.sign(p)))\n",
    "\n",
    "    return {\"rmse\": rmse, \"spearman\": spearman, \"hit\": hit}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ============================================================\n",
      "[INFO] JPX 30-Day Horizon - Expanding Window + Tuning\n",
      "[INFO] ============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "log(\"=\" * 60)\n",
    "log(\"JPX 30-Day Horizon - Expanding Window + Tuning\")\n",
    "log(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading all data sources...\n",
      "[INFO] Loaded data sources: ['stock_prices', 'secondary_stock_prices', 'options', 'trades', 'financials']\n",
      "[INFO] Building full feature table...\n",
      "[INFO] Built feature table: 2324923 rows, 34 features\n",
      "[INFO] Loaded: 2324923 rows\n",
      "[INFO] Features: 34\n",
      "[INFO] Target: target_30d\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data, feature_cols, target_col = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting hyperparameter tuning...\n",
      "[INFO] Testing 12 param combinations...\n",
      "[INFO] Best params: {'n_estimators': 300, 'learning_rate': 0.02, 'max_depth': 8, 'num_leaves': 15}, Score: 0.0510\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameters\n",
    "tuned_params = tune_hyperparameters(data, feature_cols, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running all-data-for-training, 2021-only prediction...\n",
      "[INFO]   Year 2021: Train 1,868,534 (all prior years), Test 390,383\n",
      "[INFO] Total predictions: 390,383\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "pred = predict_with_expanding_window(data, feature_cols, target_col, tuned_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved: output_train_30d\\predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Save predictions\n",
    "pred_path = os.path.join(OUTPUT_DIR, \"predictions.csv\")\n",
    "pred.to_csv(pred_path, index=False)\n",
    "log(f\"Saved: {pred_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \n",
      "========================================\n",
      "[INFO] PREDICTION METRICS\n",
      "[INFO] ========================================\n",
      "[INFO] RMSE: 0.158231\n",
      "[INFO] Spearman: 0.0577\n",
      "[INFO] Hit Ratio: 52.84%\n",
      "[INFO] \n",
      "========================================\n",
      "[INFO] PORTFOLIO METRICS\n",
      "[INFO] ========================================\n",
      "[INFO] Rebalance Days: 10\n",
      "[INFO] Total Spread: 16.4039%\n",
      "[INFO] Sharpe: 1.2467\n",
      "[INFO] Hit Ratio: 49.73%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "port_metrics = evaluate_portfolio(pred)\n",
    "pred_metrics = evaluate_predictions(pred)\n",
    "\n",
    "log(\"\\n\" + \"=\" * 40)\n",
    "log(\"PREDICTION METRICS\")\n",
    "log(\"=\" * 40)\n",
    "log(f\"RMSE: {pred_metrics['rmse']:.6f}\")\n",
    "log(f\"Spearman: {pred_metrics['spearman']:.4f}\")\n",
    "log(f\"Hit Ratio: {pred_metrics['hit']:.2%}\")\n",
    "\n",
    "log(\"\\n\" + \"=\" * 40)\n",
    "log(\"PORTFOLIO METRICS\")\n",
    "log(\"=\" * 40)\n",
    "log(f\"Rebalance Days: {port_metrics['num_days']}\")\n",
    "log(f\"Total Spread: {port_metrics['spread']:.4%}\")\n",
    "log(f\"Sharpe: {port_metrics['sharpe']:.4f}\")\n",
    "log(f\"Hit Ratio: {port_metrics['hit_ratio']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \n",
      "========================================\n",
      "[INFO] MONTHLY SPREAD\n",
      "[INFO] ========================================\n",
      "[INFO]   2021-01: Spread=-3.8335%, Hit=43.75%\n",
      "[INFO]   2021-02: Spread=-0.5575%, Hit=46.75%\n",
      "[INFO]   2021-03: Spread=+2.7768%, Hit=53.00%\n",
      "[INFO]   2021-04: Spread=+1.8886%, Hit=48.75%\n",
      "[INFO]   2021-05: Spread=+3.6544%, Hit=53.00%\n",
      "[INFO]   2021-06: Spread=+2.1528%, Hit=48.00%\n",
      "[INFO]   2021-07: Spread=+2.8395%, Hit=55.25%\n",
      "[INFO]   2021-08: Spread=+2.4316%, Hit=50.25%\n",
      "[INFO]   2021-09: Spread=+4.9145%, Hit=47.75%\n",
      "[INFO]   2021-10: Spread=+0.1368%, Hit=50.75%\n",
      "[INFO] \n",
      "Positive months: 8/10\n"
     ]
    }
   ],
   "source": [
    "# Monthly breakdown\n",
    "if \"daily_df\" in port_metrics and not port_metrics[\"daily_df\"].empty:\n",
    "    daily_df = port_metrics[\"daily_df\"]\n",
    "    log(\"\\n\" + \"=\" * 40)\n",
    "    log(\"MONTHLY SPREAD\")\n",
    "    log(\"=\" * 40)\n",
    "    daily_df[\"month\"] = pd.to_datetime(daily_df[\"date\"]).dt.strftime(\"%Y-%m\")\n",
    "    for _, row in daily_df.iterrows():\n",
    "        log(f\"  {row['month']}: Spread={row['spread']:+.4%}, Hit={row['hit_ratio']:.2%}\")\n",
    "    positive_months = (daily_df[\"spread\"] > 0).sum()\n",
    "    log(f\"\\nPositive months: {positive_months}/{len(daily_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] \n",
      "Done! Time: 1.0 min\n"
     ]
    }
   ],
   "source": [
    "# Save metrics\n",
    "metrics = {**pred_metrics, **port_metrics}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_path = os.path.join(OUTPUT_DIR, \"metrics.csv\")\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "log(f\"\\nDone! Time: {total_time/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements:\n",
    "1. **Data Loading**: Load stock prices, options, trades, and financials\n",
    "2. **Feature Engineering**: Extract technical indicators, fundamental factors, and sentiment features\n",
    "3. **Hyperparameter Tuning**: Grid search with time-series cross-validation\n",
    "4. **Model Training**: LightGBM regressor + classifier hybrid\n",
    "5. **Prediction**: Expanding window for 2021 test year\n",
    "6. **Evaluation**: Portfolio metrics (Sharpe ratio, spread) and prediction metrics (RMSE, Spearman)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shijue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
